{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook trains a defender agent with PPO\n",
    "\n",
    "This notebook offers example code on how to train a defender agent on the ESS environment with PPO. Note that for the code to work correctly, you'll need the modified versions of gym and OpenAI baselines installed (we recommend on a virtual environment). \n",
    "\n",
    "Links to modified gym/baselines:\n",
    "\n",
    "\n",
    "https://github.com/rubai5/baselines\n",
    "\n",
    "\n",
    "https://github.com/rubai5/gym\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "\n",
    "from mpi4py import MPI\n",
    "import os.path as osp\n",
    "import gym, logging\n",
    "from baselines import logger\n",
    "\n",
    "from baselines.ppo1 import pposgd_simple_generalization, mlp_policy\n",
    "import baselines.common.tf_util as U\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Game Paramters\n",
    "The ESS game has a huge number of possible states. The gym environment has some ways of sampling from these states, and here, we set the parameters to mix the distributions as desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "name = \"ErdosGame-v0\"\n",
    "seed = 101\n",
    "\n",
    "# game specific parameters\n",
    "K = 15\n",
    "potential = 0.9\n",
    "\n",
    "# sampling probabilities, must sum to 1\n",
    "unif_prob = 0.0\n",
    "geo_prob = 1.0\n",
    "diverse_prob = 0.0\n",
    "state_unif_prob = 0.0 # can only use if K is small < 10 -- try to use previous methods instead\n",
    "\n",
    "assert (unif_prob + geo_prob + diverse_prob + state_unif_prob == 1), \"probabilites don't sum to 1\"\n",
    "\n",
    "# attacker plays adversarially?\n",
    "adverse_set_prob = 0.0\n",
    "disj_supp_prob = 0.5\n",
    "\n",
    "# high one\n",
    "high_one_prob = 0.0\n",
    "\n",
    "# upper limits for start state sampling\n",
    "geo_high = K - 2\n",
    "unif_high = max(3, K-3)\n",
    "\n",
    "# putting into names_and_args argument\n",
    "names_and_args = {\"K\" : K, \"potential\" : potential, \"unif_prob\" : unif_prob, \"geo_prob\" : geo_prob,\n",
    "                   \"diverse_prob\" : diverse_prob, \"state_unif_prob\" : state_unif_prob, \n",
    "                  \"high_one_prob\" : high_one_prob, \"adverse_set_prob\" :adverse_set_prob, \n",
    "                  \"disj_supp_prob\" : disj_supp_prob, \"geo_high\" : geo_high, \"unif_high\" :unif_high }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "HID_SIZE=300\n",
    "NUM_HID_LAYERS=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Net, Train and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to initialize environment and train model\n",
    "\n",
    "def policy_fn(name, ob_space, ac_space):\n",
    "        return mlp_policy.MlpPolicy(name=name, ob_space=ob_space, ac_space=ac_space, \n",
    "                                    hid_size=HID_SIZE, num_hid_layers=NUM_HID_LAYERS)\n",
    "    \n",
    "def make_policies(ob_space, ac_space, policy_func):\n",
    "    pi = policy_func(\"pi\", ob_space, ac_space)\n",
    "    oldpi = policy_func(\"old_pi\", ob_space, ac_space)\n",
    "    \n",
    "    return pi, oldpi\n",
    "\n",
    "def train(env_train, pi, oldpi, names_and_args, num_timesteps, test_envs):\n",
    "    #workerseed = seed + 10000 * MPI.COMM_WORLD.Get_rank()\n",
    "    #set_global_seeds(workerseed)\n",
    "    \n",
    "    env_train.reset()\n",
    "    if test_envs:\n",
    "        for test_env in test_envs:\n",
    "            test_env.reset()\n",
    "    \n",
    "    #env.seed(workerseed)\n",
    "    gym.logger.setLevel(logging.WARN)\n",
    "  \n",
    "\n",
    "    policy_net, info = pposgd_simple_generalization.learn(env_train, pi, oldpi,\n",
    "        max_timesteps=num_timesteps,\n",
    "        timesteps_per_batch=100,\n",
    "        clip_param=0.2, entcoeff=0.01,\n",
    "        optim_epochs=4, optim_stepsize=1e-3, optim_batchsize=50,\n",
    "        gamma=0.99, lam=0.95,\n",
    "        schedule='linear',\n",
    "        test_envs=test_envs\n",
    "    )\n",
    "\n",
    "    return policy_net, info\n",
    "\n",
    "\n",
    "def test_policy(num_rounds, policy_net, test_env):\n",
    "    total_reward = 0.0\n",
    "    horizon = test_env.observation_space.K*num_rounds # generate around num_rounds draws\n",
    "    seg_gen = pposgd_simple_generalization.traj_segment_generator(policy_net, test_env, horizon, stochastic=True)\n",
    "    \n",
    "    # call generator\n",
    "    results = seg_gen.__next__()\n",
    "    mean_reward = np.mean(results[\"ep_rets\"])\n",
    "    actions = results[\"ac\"]\n",
    "    labels = results[\"label\"]\n",
    "    mean_correct_actions = compute_correct_actions(labels, actions)\n",
    "    return mean_reward, mean_correct_actions\n",
    "\n",
    "def compute_correct_actions(label, ac):\n",
    "    count = 0\n",
    "    idxs = np.all((label == [1,1]), axis=1)\n",
    "    count += np.sum(idxs)\n",
    "    new_label = label[np.invert(idxs)]\n",
    "    new_ac = ac[np.invert(idxs)]\n",
    "    count += np.sum((new_ac == np.argmax(new_label, axis=1)))\n",
    "    avg = count/len(label)\n",
    "    return avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on sessions\n",
    "To run most of the baselines code, we need to explicitly state that the session is the default one, i.e. start with\n",
    "    <code here>\n",
    "    with sess.as_default():\n",
    "    </code here>\n",
    "The code is currently set up for initializing sess = U.single_threaded_session() as a global variable and closing/reseting the graph explicitly to enable restarts, etc. Note that U.reset() must be used along with tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to load graphs and sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "def reset_session_and_graph():\n",
    "    try:\n",
    "        sess.close()\n",
    "    except:\n",
    "        pass\n",
    "    tf.reset_default_graph()\n",
    "    U.reset()\n",
    "    \n",
    "def save_session(fp):\n",
    "    # saves session\n",
    "    assert fp[-5:] == \".ckpt\", \"checkpoint name must end with .ckpt\"\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, fp)\n",
    "    \n",
    "def load_session_and_graph(fp_meta, fp_ckpt):\n",
    "    saver = tf.train.import_meta_graph(fp_meta)\n",
    "    saver.restore(sess, fp_ckpt)\n",
    "    U.load_state(fp_ckpt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train network over a number of repeats\n",
    "repeats = 3\n",
    "SAVE_FP = \"/tmp/\"\n",
    "\n",
    "\n",
    "for K in [25]:\n",
    "    for potential in [0.99]:\n",
    "        names_and_args[\"K\"] = K\n",
    "        names_and_args[\"geo_high\"] = K-2\n",
    "        names_and_args[\"unif_high\"] = max(3, K-3)\n",
    "        names_and_args[\"potential\"] = potential\n",
    "        rewards = []\n",
    "        test_rewards = []\n",
    "        for rep in range(repeats):\n",
    "            reset_session_and_graph()\n",
    "            sess = U.single_threaded_session()\n",
    "\n",
    "            with sess.as_default():\n",
    "                erdos_env = gym.make(name, **names_and_args)\n",
    "                pi, oldpi = make_policies(erdos_env.observation_space, erdos_env.action_space, policy_fn)                \n",
    "                \n",
    "                pi, info = train(erdos_env, pi, oldpi, names_and_args, num_timesteps=50000, \n",
    "                                 test_envs=[])\n",
    "                rewards.append(info[\"rewards\"])\n",
    "                \n",
    "                # save model\n",
    "                model_fp = SAVE_FP+\"model_K%02d_potential%f_rep%02d.ckpt\"%(K, potential,\n",
    "                                                                             rep)\n",
    "                save_session(model_fp)\n",
    "\n",
    "        # save results\n",
    "        with open(SAVE_FP+\"rewards_K%02d_potential%f.p\"%(K, potential), \"wb\") as f:\n",
    "            pickle.dump(rewards, f)            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
